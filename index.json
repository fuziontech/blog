[{"content":"","date":"25 August 2021","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"25 August 2021","permalink":"/categories/dataquality/","section":"Categories","summary":"","title":"dataquality"},{"content":"What does the future PostHog Platform look like? # This is a rough outline of what I would build given unlimited resources currently.\nSupport. - The most time consuming thing we will need SREs for is simply managing on prem PostHog deployments. If people are paying us for PostHog the future is going to be filled with N+1 users looking for: Supporting and debugging issues Helping with upgrades Performance tuning Migrations License management ðŸ¤£ The more we can automate getting ahead of all of this, the better. K8s installs Support each of the following environments better: AWS GCP Azure DO IBM Metal Each of these environments have different levers we can pull. Each one is relatively similar in shape, but the amount of work to research, test, and implement is relatively large if we want to support truly large deployments. Potentially split out ClickHouse from our chart. This would be relatively easy to do at first for a handful of customers (basically bespoke) but eventually it would be nice to have the same set of tools we build for our cloud available to customers for scaling up ClickHouse outside of K8s. Cloud infrastructure Automate ClickHouse cluster management Resharding of sharded tables This is a big one as it is relatively bespoke the way that ClickHouse suggests running resharding jobs. Basically with ClickHouse Copier. Not exactly something we can run on customers datasets. I\u0026rsquo;d like to have this be automated. It may not be possible though. Backups and point in time restores Our backups still need a lot of work. Compliance SOC2 Type I SOC2 Type II HIPAA GDPR Offering Data Processing Addendums CCPA Offering Data Processing Addendums General security improvements (there are a lot) Restrict public access to production Proxy access to internal tools through a single auth source (GitHub, Google, etc) Proxy production access the same way Audit all access Single tenant support This is a big one at a similarly positioned company. They have a dedicated stack that they do not want to manage and pay top dollar for a managed cloud version. Regional installations US EU China Russia Asian Pacific Shared infrastructure - Cloud and on prem improvements Managing and migrating table schemas In order to iterate quickly we are going to need some way to manage the schemas and the migrations of the tables on ClickHouse. We do already have migrations in PostHog, but that\u0026rsquo;s only for simple and fast migrations. This would have to be something more robust to support long running queries on ClickHouse. Something that might need to use ClickHouse Copier Object stores Person store Eventually we will need a person store as we grow out of Postgres. This won\u0026rsquo;t happen for a long time, but it is very possible it will happen at some point in the future. It may also become an issue where an on prem deployment is unable to manage or scale up a postgres instance. Event store If we are ever to append persons to events to avoid joins we will need to have a cannonical event store that lives outside of ClickHouse. We explored this earlier this year and decided against it because we were using Dyanmodb, probably for the best. Both of these would most likely live in something like ScyllaDB but more research is needed. Maybe we use Vitess? Backups Point in time restores for paying customers. Basically have the ability to replay events from some storage: Block store like EBS (or equivalent) Blob store like S3 (or equivalent) Improve admin dashboard We should invest in better self service tooling for managing PostHog. The internal_metrics dashboard has been a great start, but there is so much more that we can do. Plugin service metrics Events metrics Task metrics Plugin metrics Event -\u0026gt; Plugin factor Events service metrics Events / second 200s/400s/500s Kafka metrics Latency Disk usage Topic metrics IO metrics Consumer Group latency ZK metrics If ZK is a critical point of failure. ClickHouse Operator metrics Status of backups Beam me up! / Beam me down! - Moving to cloud and back Some seamless way to migrate everything from cloud and back. This includes Action definitions Cohort definitions Insights Dashboards Plugin configurations (if able) Session recordings Events Persons \u0026amp; Distinct IDs Feature Flags Experiments Organization and Teams w/ Users (if able) Testing of other infrastructure deployment vectors We did not use K8s at Uber, so right now if an engineer wanted to setup PostHog to instrument an interal app they would be SOL. We should investigate other means to deploy PostHog. This will increase the complexity of managing our infrastructure, but if we are serious about PostHog as a product that will be deployed internally this might be something we will have to support. (open product deployment strategy discussion) Is just a big Terraform what we might need here? Is there a better way to deploy PostHog if K8s is not available? Prototyping different infrastructure options. - The ecosystem of infrastructure options that exist to us is huge and constantly changing. We should be looking at better options as we go. Vectorized RedPanda New versions of ClickHouse ScyllaDB for object stores Vitess PostHog Operator for K8s? Leveraging PostHog - for complete monitoring and alerting of infrastructure related issues. This is a big one but something we should be striving for long term. What we build here could be huge in the future. We talked about this a while back and dismissed it as too hard. If we are larger and allocate resources to this - it is feasible. Metrics by dimensions Outlier/Anomaly detection Forecasting Holt-winters Linear regression so much to do! Triggering events/actions/alerts based on arbitrary rules Stream Processing - This piggy-backs on the last line item. If we build out the API needed for triggering events on arbitrary rules we can build out a stream processing pipeline. Triggering events/actions/alerts to external systems based on arbitrary rules, aggregates, predictions, outliers Consider building out Distributed RPC systems using plugins as a basis Hit this API and get a wholistic view of a user from PostHog, Hubspot, salesforce, and some internal API Submit a certain type of event and call a series of downstream RPC endpoints Build out better examples of building PostHog into core streaming data workflows in apps. Platform Integrations - More ways to onboard to PostHog. We should write as many docs as possible to help gain developer and product manager mindshare but the truth is the easier we make it to onboard to PostHog cloud from a place someone mine discover us the better. Vercel Heroku Netlify Data Engineering tools for PostHog. \u0026lt;- We have SREs applying with DE backgrounds. Perfect. We have talked about this earlier with plugin exports but this would be dedicated, opinionated examples on how to load data from things like: Hive - needs Java Client library as UDF Hadoop - needs Java Client library Spark - needs Java Client library Presto/Trino - needs Java Client library as UDF Druid - needs Java Client library Airflow - Python âœ… Snowflake - Library TBD DBT (for modeling data into the correct schema) Luigi - Python âœ… Snowplow - Library TBD S3 - Using a task orchestrator like Airflow/Luigi Solid documentation on: What the schema needs to be for Events and Persons (from Data Engineering Perspective) Updating person properties and attributes effectively Filling in event gaps by using your data warehouse Backfilling event data (rewriting events) Client Libraries - We still have a few to build Java (We have Android!) Java UDFs for Data Warehouses C/C++ - Critical for IoT Rust C# Scala (using Java lib) - Just example and docs Clojure (using Java lib) - Just example and docs Kotlin (using Java lib) - Just example and docs Pluggable backends - Remember, unlimited funding. Make it possible to replace the backend of PostHog with a different OLAP database. I\u0026rsquo;m still betting my money on ClickHouse, but to land certain deals this would be a deal-making option. Presto/Trino Druid Snowflake BigQuery ClickHouse Stewardship - Contributing back to ClickHouse Help steer the roadmap for ClickHouse to better align with our long term goals. Speed up the development of features that are on the horizon that we need sooner than later Window functions Materialized Columns - force materialization Better resharding and schema mutations Change table engine in place Change sort by keys in place Change sample by key in place Bug fixes Performance improvements Improve generic tooling around ClickHouse ClickHouse Operator ClickHouse Copier ClickHouse Backup Automated upgrades - for self-hosted (Jams mentioned this in the past). Currently upgrades are manual, which leaves many self hosted folks running old versions of PostHog (basically install once, upgrade never pattern) and they just don\u0026rsquo;t get the cool new stuff we built nor any of the perf improvements. Jams also mentioned that some other company had the same problem \u0026amp; perhaps we could have a PostHog operator, that fetches updates and applies them. ","date":"25 August 2021","permalink":"/posts/2021-08-25-posthog_infra/","section":"Posts","summary":"What does the future PostHog Platform look like?","title":"Future PostHog Platform"},{"content":"","date":"25 August 2021","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"25 August 2021","permalink":"/","section":"State of the Union","summary":"","title":"State of the Union"},{"content":"\u0026ldquo;Why Software Is Eating the World\u0026rdquo; - Marc Andreessen \u0026ldquo;Data Scientist: The Sexiest Job of the 21st Century\u0026rdquo; - DJ Patil Software is in fact eating the world. But why is it eating the world? It\u0026rsquo;s because generally software reduces friction points in almost any process or product that doesn\u0026rsquo;t have software. Teams building software are looking to build products that make users lives\u0026rsquo; easier.\nSo how do we build products that make users lives\u0026rsquo; easier? Well before software you would have focus groups, surveys, and user interviews. Theses were effective means to gain a sample of understanding of how users are using your product. Learning where the friction points are. What was confusing.\nSoftware is eating the world though. So it is only a matter of time before all of this is automated as well. Meaning that eventually most of this will be explicitly collected by software which means the scale of the data collected explodes. We collect product events, interactions, intents and store them for later. So that we can build better products.\nWhat value does data bring? # Ask any Product Manager and their answer will be to better understand how people are using the product. How to get users to convert to paying users How to get paying users to stick around and get value out of the product How to stem the bleeding of churned users Sound good? Do I have you sold yet? But what if, just like software, something breaks in your data? Incorrect concusions are drawn. The product gets worse. The software loses some ground. A user is frustrated and churns.\nThis is why having a means to monitor the quality of your data is so important. As with any system: Garbage in = Garbage out. So how does a team monitor their data for quality? What even is quality when it comes to data. We all know how to check if a banana at the supermarket is bad. Or if the bread in the kitchen is bad. What characteristics does good data have and bad data lack?\nWhat is data quality? # Latency Accuracy Precision Completeness Data Latency # Data latency is the easiest feature to define. It\u0026rsquo;s how old is the data we are looking at? What is the delta between now and state of the data being represented?\nAccuracy # Accuracy is how faithfully does the data represent reality. Example of bad accuracy would the timestamp field says that an event happened at X time but in reality it happened 4 hours before then because of timezones or a bad clock. For web collected data there are about a million ways things can go sideways between the browser and wherever data is getting stored so you may find a significant % of data has malformed DOM elements if you are recording page interaction.\nPrecisions # Precision of data is does the data have the correct data type to represent the data correctly. If you run a HFT firm and you are studying transactions you need to have the transaction data have the correct precision for timestamps. Cutting timestamps off at the seconds is a dealbreaker. The data is Accurate, but not Precises. Meaning that it is bad data for that use case.\nCompleteness # Completeness of data is whether or not the data is lossy. Fields missing or entire rows missing would be incomplete data.\nHow do you set expectations about data quality? # SLA - Service Level Agreement Latency - Latency Guarantees Accuracy - Tests Pass Rate Precision - Type checking \u0026amp; Tests Pass Rate Completeness - Tests Pass Rate Ensuring Quality # For inspiration on how we can ensure that teams have quality data we can look at a group of people who have been struggling with this problem since the very beginning: Software Engineers. Building software is the exact same as building data. It is riddled with bugs, mistakes, and breaking changes. What you have to do in order to make any progress in this world is to code defensively. You know you are going to make a mistake. Instead of trying to detect that mistake yourself build some software that will detect if for you! and that\u0026rsquo;s how tests were born.\nSo what the hell do tests for data look like?\nWell to be frank, quite a bit like tests for software.\nKinds of tests # Rule Based Tests Completeness Precision Latency Static Analysis Precision Unit Test Accuracy Precision Completeness Rule Based Tests # Rule based tests are the simplest of all of these tests. They basically just check to see if things are within bounds. Is the created_at field in the period that you expect? Like is it within the last 15 minutes? Does every column contain values? Are there any gaps of rows where there shouldn\u0026rsquo;t be? This is the kind of test where you can apply machine learning and infer what the distribution of data should look like and so based on the rules of the model you can alert on any outlier.\nRules based tests can also be used to ensure the precision of the data. By testing basic rules like if a UInt64 always has the precision of a UInt32, it might have been downsampled upstream and need to be flagged. Or if a varchar(255) always has exactly 255 characters used you might be truncating your string value.\nStatic Analysis # This one is relatively easy to implement because you are only really looking at the schema. You want to make sure that all columns and fields have the datatype that is necessary for correct Precision. Is a UInt64 field always a UInt64 field? Does the query logic being used to fill keep the precision correct from start to end? If it doesn\u0026rsquo;t you might be trusting a UInt64 field that was really downsampled to a UInt16 field and then brought back up.\nUnit Tests # This is probably the most valuable of the tests, just like with building software. Since most data being build is basically taylor made along with the software that\u0026rsquo;s producing it no one knows better about what looks right or wrong than the person building the logic! This is why unit tests are so great in software. As the engineer you know that for your add(x, y) function add(1,1) will always return 2. You can write that up as a rule protecting you from your future self. This is what we need in data.\nA data team who invests the time in creating these tests will be saving themselves loads of time later trying to track down bugs. It also reduces the time to detecting issues since the test suite will pick up the regression or the bug almost immediately if it is setup correctly. As a newly onboarded data team member having these tests reduce the barier to entry since there is a security blanket associted with a test suite. You can push a change with the confidence of knowing that what you just pushed will not silently break something.\nSo why do we want to do all of this? # There are so many reasons why you should invest in your Data Quality infrastructure. At it\u0026rsquo;s core though, it is about confidence in the data. Just as with people, trust is hard earned and easily lost. If your team loses confidence in the data being collected they won\u0026rsquo;t use it. It will no longer be a trusted ally but a deadbeat that never has your back.\nProducts with Data Quality Management features # BigEye Soda.io great_expectations Socrata talend mobyDQ AWS Glue DataBrew \u0026lt;- more munging of data DBT Appendix: # Data SRE Trust @ Uber DBT Tests ","date":"17 November 2020","permalink":"/posts/2020-11-17-data-quality/","section":"Posts","summary":"\u0026ldquo;Why Software Is Eating the World\u0026rdquo; - Marc Andreessen \u0026ldquo;Data Scientist: The Sexiest Job of the 21st Century\u0026rdquo; - DJ Patil Software is in fact eating the world.","title":"You wouldn't trust bad advice. Don't trust bad data!"},{"content":"","date":"6 November 2020","permalink":"/categories/hiring/","section":"Categories","summary":"","title":"hiring"},{"content":"Hiring is hard # Hiring is very hard. Somehow you have to separate the wheat from the chaff while minimizing the cost of doing that on your team. The cost of a bad hire is super high. So you have to also maximize the likelihood that the hire will be a successful one. You also need to be respectful to the candidate and not waste their time. What is discussed here is specifically for an engineering hire, but could be adapted to any hire with a few modifications.\nCharacteristics of a good hiring process:\nRespectful of team\u0026rsquo;s time Respectful of the candidate\u0026rsquo;s time Determines the technical capabilities of the candidate Gauge the experience of the candidate Judge if the candidate is a good cultural fit Is consistent while scaling Correlates as closely as possible with day to day work performance Fail fast # Phone Screen # The first thing to keep in mind is that your team\u0026rsquo;s time is gold. It is the most precious resource any of us have and that applies to your everyone at your company as well. Your process needs to be able to filter out who is not a good fit as early as possible. The process should also be able to do that with the least touch possible from people on your team. This means your process must be built to fail fast.\nSome of the best processes that I\u0026rsquo;ve seen personally involve a technical phone screen that has just one person asking a generally basic question that estimates if the person is even in the realm of what you are looking for. It is a great spot to check, at least superficially, what the candidate has listed on their resume. They list that they have 10 years of ruby - do they know how to a relatively simple task in ruby? Better for you and the candidate to find out now with just their counterpart over zoom.\nHomework # A solid alternative to the phone screen is homework. This gives you even more signal on whether or not the candidate is worth bringing onsite and spending effectively an engineering work day on. The main downside is that it can be a waste of time for the candidate if they turn out to not submit something that your team is keen on.\nDesign your system with this as a key requirement. If your process can\u0026rsquo;t fail fast you will end up spending a great deal of your team\u0026rsquo;s time and leave a sour taste in the mouths of your candidates\nThink Scale and Consistency # Your system has to be able to scale, for a few reasons. You might need to grow your team rapidly because of some new opportunity. You may have several world class candidates show up at your doorstep that you want to get in the door quick. You maybe be huge and want to have a consistent experience for all teams to ensure consistent quality. It\u0026rsquo;s important to build out your process with this in mind.\nSome places like to give homework. Others do a phone screen. Some have you do homework, bring it in, and talk about it during the onsite. A select few places contract you and work with you for a day or a week to establish just how it is to work with you.\nSo far this is what we have as a process:\nTechnical Phone screen Onsite Debrief You can ignore this advice for your founding team; for that you will want something as high touch as possible to make sure you are making the right hire and are able to close on the people who you believe will either make or break your organization.\nThe onsite # So you\u0026rsquo;ve found someone who looks great on paper, and has passed the first hurtle of the technical phone screen or their homework looks great. Now it\u0026rsquo;s time to bring them onsite to have the team meet and test them. You not only want to take this opportunity to test the candidate and for your team to meet and get an impression of them. You want to be able to use this time to sell the candidate on what you are doing, your mission, and paint a picture of where they would fit in your org.\nYou want some representative chunk of the hiring team to meet the candidate and have the opportunity to interact and test them. If you follow Amazon\u0026rsquo;s two-pizza rule on team size this shouldn\u0026rsquo;t be too much of a problem between the interviews and some subset of the team going to lunch with the candidate. As for the interview sessions. Something that I have seen work very well is breaking the session down into about 3 technical chunks about an hour long.\nCoding \u0026amp; Debugging # In this session the interviewer will ask some relatively mundane task be complete with the goal being to see how the candidate breaks the problem down and attacks it. Where do they get stuck? How do they unstick themselves. I personally like to work somewhat collaboratively with the candidate to see where they ask for help. This is a great way to see if they will be someone who will get stuck and immediately defer to someone else or are they someone who will sit and spin their wheels forever. Ideally it\u0026rsquo;s somewhere in between. Where the exact line is? That is for the interviewer to decide.\nI also like to see how the candidate captures the requirements of the problem. To date my favorite interview was where the candidate captured the requirements as a test suite and built out the solution against that. There was absolutely no questions after we first settled on the suite whether or not the solution accomplished the goal we set out with.\nSystems Architecture # Systems architecture was one of my favorite questions to ask as an interviewer mainly because there really is no limit to how deep you can go with the question. I always liked starting out by telling the candidate that I was some \u0026lsquo;mom and pop shop\u0026rsquo; who needed just a solution that worked and was fast to build. Something that would be cheap. From there we would run the solution out until we were the size of Amazon or Apple. It was generally fun for both me and the candidate as we picked things apart and asked questions. It\u0026rsquo;s a really great session to work collaboratively.\nMore often than not you could identify what technologies people had used before because of their familiarity with where they fit in the grand scheme of things. I loved digging into both what they were comfortable with and with what they were not. With what they were comfortable with you really see where their experience would be a value add. With where they were not you could see how intuitive they were. It was a ton of fun. I think for most people this was their favorite session to participate in, on both sides\nAlgorithms # Ah, algorithms. How often do you actually use the things you are tested in day to day? In reality, almost never. However, I still think it\u0026rsquo;s important to have a panel on this. The reason why I think it is still important is a combination of table stakes, and respect. Table stakes because this is one of the sessions I expect the candidate to study for. This shows me some level of preparation. The other is a respect for what is happening behind the scenes of a library. Honestly, who out there re-developes a sorting algorithm. Who has to write their own sorting algorithm or new way to build out a trie? Not many of us and not often. But asking this question gives you a good estimate of how the candidate thinks about efficiencies.\nEven if a candidate doesn\u0026rsquo;t know how to give you the perfect solution, this is where you can probe to see if they have an intuition on where the slow part of their code might exist. This is also the section where you can see if, given a relatively tough objective, can they even get there. I never really penalized a candidate if they were able to get to the objective and were able to explain where things were slow. It was only extra points if they could rationalize how to make things faster and the tradeoffs of different solutions.\nLunch with team # This one is crucial. The team has to have lunch with the candidate. If you are remote and can\u0026rsquo;t have lunch in person, at least schedule some social time to just hang out with the candidate and ask the softer questions: What do you do for fun? How do you spend your free time. What sorts of hobbies are you into. This gives the candidate some time to relax and get out of test mode and also is a great way to see if they are someone who is a good culture fit for the team.\nMuch overlooked, this is where you sell the candidate on your team as well. This where you want to make the candidate think, yeah I could work with these people every day. Talk about the future of the product, what you find interesting. Get them engaged. It\u0026rsquo;s so important. The candidate will remember it when they are considering their options later.\nHiring manager # So far things are looking good. Time to meet the hiring manager. Honestly this can happen at any time. Often times it can even be the first meeting of the day. What you will be looking to do during this is gauge how they are as a team member. How do they de-conflict at a crossroads? What are they looking for in their career? Do they have the experience and capacity to mentor more junior members of the team or will they need to be mentored.\nMost importantly, this is where the hiring manager will contextualize the position and why it is important to the organization. This is also where I would expect the candidate to ask about more procedural things like, are there sprints? How are they run? Are product requirements purely top down or is it a bottoms up product development cycle?\nI generally also probe their resume and ask about previous projects. What went well? What didn\u0026rsquo;t go so well? See if they can be honest about learning and iterating.\nBar Raiser # This was something that we did quite successfully at Uber and it was a total asset. A cultural belief of ours was that every hire should \u0026lsquo;raise the bar\u0026rsquo; as compared to the last. The idea being that if you do that you will only hire the best and never settle. This of course is aspirational, but with the right technique you can at least mitigate the settling part. How we did this at Uber was to have someone from a totally separate team participate in the process. They had no horse in the race. They were there only as a neutral participate in the process. They had their own session similar to the hiring managers. It was to feel out what drove the candidate. What were their motivations? Were they going to raise the bar at Uber?\nThe other responsibility for the Bar Raiser was to drive the debrief which we will get to later. What you need to know now was they didn\u0026rsquo;t have any real power to say yes. On the contrary, they only had the ability to say no. If they felt that the hire would not raise the overall talent bar, then they could say no and that was final. This was mainly to guard against a desperate hire where a manager just needed someone to have a butt in a seat.\nRecap # The onsite interview looks something like this so far:\nCoding \u0026amp; Debugging Systems Architecture Algorithms Lunch with team Hiring manager Bar Raiser Debrief # So after all of this you\u0026rsquo;ve had a number of people on your team, and not on your team, meet the candidate to see if they will be a solid addition to the team. Now it\u0026rsquo;s time to make a decision. The best way to do this is to not have anyone talk about the results of the process. You don\u0026rsquo;t want to introduce any bias into the system. No participant in the interview process should talk about the performance until everyone sits down in a room for the specific purpose of the debrief. What happens otherwise is you will introduce some bias between interviewers. As an example one interviewer might have been super impressed with the candidate during their session which might bias their friend, who did another panel, into submitting a slightly higher score than they would have otherwise. Let\u0026rsquo;s say it just moved them from being a single thumbs down into being a single thumbs up. You want to avoid this so that during the debrief you get as raw of an impression as possible, then you hash that out.\nOne of the more successful processes I\u0026rsquo;ve seen is you get all of the participants into a room and start off with a thumbs up / thumbs down. Two thumbs down is an absolutely not. Two thumbs up is a top 5-10% of candidates ever. We need to hire this person. Once everyone reveals their position you talk about the notes that everyone\u0026rsquo;s taken on the interview and have everyone justify their decision.\nSomething that is interesting is, at Uber, the debrief process is driven by the Bar Raiser. They are the MC of the debrief and make sure that there is no bias and the interview panel was ernest in their questioning of the candidate. They are the \u0026lsquo;poll watchers\u0026rsquo; of the process.\nSummary # So far this is what we got:\nTechnical Phone screen Onsite Coding \u0026amp; Debugging Systems Architecture Algorithms Lunch with team Hiring manager Bar Raiser Debrief This accomplishes most of the goals we set out towards:\nRespectful of teamâ€™s time Respectful of the candidateâ€™s time Determines the technical capabilities of the candidate Gauge the experience of the candidate Judge if the candidate is a good cultural fit Is consistent while scaling Correlates as closely as possible with day to day work performance We minimize the cost of time spent both by the candidate and the team on figuring out if there is a mutual fit early. We determine their technical capabilities in a few dimensions ranging from practical to academic. We were able to gauge their experience with blocks of architecture as well as working on a team. We broke bread with the candidate and got a feel for what motivates them. The process scales linearly. However, the toughest one to nail is correlating this process to day to day performance. I still haven\u0026rsquo;t seen a perfect test of this short of contracting with the candidate for the better part of a week, but that breaks the goal of minimizing the time invested for the team and for the candidate.\nI still do believe that with this framework you get enough discrete signal in enough dimensions to make a judgement call on whether or not the candidate will be a good performer on the work they will be doing day to day. Even better than more realistic work in some ways since you will be able to concretely measure how they perform in the different dimensions (like great at coding but not so great at algorithms).\nAnother factor to think about is don\u0026rsquo;t worry about making the process too hard. Some places hae a process that strings out a candidate for a month plus culminating in an acceptance period where the candidate must decide yes or no in a 24 hour window. This can actually add value as it\u0026rsquo;s almost a form of hazing or initiation. Everyone has \u0026rsquo;earned\u0026rsquo; their spot at the company and will feel more included because of it. More similar than not to some super selective secret organization.\nAnyways, like I said. Hiring is hard. Let me know what you think. What are great hiring experiences that you\u0026rsquo;ve been a part of?\n","date":"6 November 2020","permalink":"/posts/2020-11-06-hiring/","section":"Posts","summary":"Hiring is hard # Hiring is very hard.","title":"How to hire only the best"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 January 0001","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]